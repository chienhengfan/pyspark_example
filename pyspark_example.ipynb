{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1be80e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv('bank-additional/bank-additional-full.csv',\n",
    "                    sep=';',\n",
    "                    header=True,\n",
    "                    inferSchema=True)\n",
    "df.show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e81e083",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 可以透過.toDF來更改column名稱\n",
    "df = df.toDF(*[c.replace('.', '_') for c in df.columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3672a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import isnull, count, when, col\n",
    "\n",
    "def data_overview():\n",
    "    \"\"\"\n",
    "    input: dataset\n",
    "    output: data summary\n",
    "    \"\"\"\n",
    "    print('Rows:', df.count())\n",
    "    print('Columns:', len(df.columns))\n",
    "    print('Missing', df.select([count(when(isnull(c), c)).alias(c) for c in df.columns]).\\\n",
    "                        toPandas().sum().sum())\n",
    "    for c in df.columns:\n",
    "        print('{:15}: '.format(c), df.select(c).distinct().count(), '\\t', \n",
    "              [row[c] for row in df.select(c).distinct().collect()[:5]])\n",
    "        \n",
    "data_overview()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c9fb807",
   "metadata": {},
   "outputs": [],
   "source": [
    "count(when(isnull(c), c)).alias(c) for c in df.columns\n",
    "# isnull表示判斷是不是null值，當是null值時，就返回該資料，其他的則被判定成nan，而在計算期column下的數量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62b94840",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count plot\n",
    "grouped = df.groupBy('job', 'y').count()\n",
    "\n",
    "import plotly.tools as tls\n",
    "\n",
    "# 透過row[feature]得到其所代表的值\n",
    "trace1 = go.Bar(x=[row['job'] for row in grouped.filter(grouped['y'] == 0).collect()],\n",
    "                y=[row['count'] for row in grouped.filter(grouped['y'] == 0).collect()],\n",
    "                name='No',showlegend=True)\n",
    "\n",
    "trace2 = go.Bar(x=[row['job'] for row in grouped.filter(grouped['y'] == 1).collect()],\n",
    "                y=[row['count'] for row in grouped.filter(grouped['y'] == 1).collect()],\n",
    "                name='Yes')\n",
    "\n",
    "# Mean Plot\n",
    "grouped_mean = df.groupBy('job').agg({'y': 'mean'})\n",
    "trace3 = go.Bar(x=[row['job'] for row in grouped_mean.collect()],\n",
    "                y=[round(row['avg(y)'], 2) for row in grouped_mean.collect()],\n",
    "                name='Mean Subscription Rate', showlegend=False)\n",
    "\n",
    "fig = tls.make_subplots(rows=1, cols=2,\n",
    "                        subplot_titles=('Count Plot of Job',\n",
    "                                        'Mean Subscription Rate of Job'))\n",
    "fig.append_trace(trace1, 1, 1)\n",
    "fig.append_trace(trace2, 1, 1)\n",
    "fig.append_trace(trace3, 1, 2)\n",
    "\n",
    "py.iplot(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e68ee24",
   "metadata": {},
   "outputs": [],
   "source": [
    "trace1 = ff.create_distplot([df.filter(df['y'] == 1).rdd.map(lambda x: x['age']).collect(),\n",
    "                            df.filter(df['y'] == 0).rdd.map(lambda x: x['age']).collect()],\n",
    "                            group_labels=['Subscription 1',\n",
    "                                          'Subscription 0'],\n",
    "                            show_rug=False)\n",
    "\n",
    "grouped_mean = df.groupBy('age').agg({'y': 'mean'}).orderBy('age')\n",
    "trace2 = go.Scatter(x=grouped_mean.rdd.map(lambda x: x['age']).collect(),\n",
    "                    y=grouped_mean.rdd.map(lambda x: x['avg(y)']).collect(),\n",
    "                    mode='lines+markers',\n",
    "                    showlegend=False)\n",
    "\n",
    "fig = tls.make_subplots(rows=1, cols=2,\n",
    "                        subplot_titles=('Count Plot of Age',\n",
    "                                        'Mean Subscription Rate vs. age'))\n",
    "\n",
    "# If you want to make distplot in the subplots,\n",
    "# you need to plot if separately after make ff.creat_distplot\n",
    "fig.add_trace(go.Histogram(trace1['data'][1],\n",
    "                           marker_color='blue',\n",
    "                           marker={'opacity':0.3},\n",
    "                           showlegend=True), 1, 1)\n",
    "fig.add_trace(go.Histogram(trace1['data'][0],\n",
    "                           marker_color='red',\n",
    "                           marker={'opacity':0.4},\n",
    "                           showlegend=True), 1, 1)\n",
    "\n",
    "fig.append_trace(trace2, 1, 2)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9701d381",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, StandardScaler\n",
    "\n",
    "continuous_features = [d[0] for d in df.dtypes if (d[1] != 'string') & (d[0] != 'y')]\n",
    "categorical_features = [d[0] for d in df.dtypes if (d[1] == 'string') & (d[0] != 'y')]\n",
    "\n",
    "# Every categorical features have own indexer\n",
    "indexers = [StringIndexer(inputCol=c, outputCol='{}_indexed'.format(c)) for c in categorical_features]\n",
    "# Depend each indexer to build encoder\n",
    "encoders = [OneHotEncoder(inputCol=indexer.getOutputCol(), \n",
    "                          outputCol='{}_encoded'.format(indexer.getOutputCol())) for indexer in indexers]\n",
    "assembler = VectorAssembler(inputCols=[encoder.getOutputCol() for encoder in encoders]+continuous_features,\n",
    "                            outputCol='features')\n",
    "\n",
    "pipeline = Pipeline(stages=indexers+encoders+[assembler])\n",
    "\n",
    "model = pipeline.fit(df)\n",
    "data = model.transform(df)\n",
    "\n",
    "data = data.withColumn('label', col('y'))\n",
    "data = data.select('features', 'label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad129de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingData, testData = data.randomSplit([0.8, 0.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84978eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_model_prediction(algo, train, test, cf='coefficient'):\n",
    "\n",
    "    model = algo.fit(train)\n",
    "    predictions = model.transform(test)\n",
    "    pred = predictions.select('label', 'prediction')\n",
    "    probabilities = predictions.select('label', 'probability')\n",
    "    \n",
    "    # Categorical Features (得到類別性變數feature names)\n",
    "    binary_metadata = predictions.select(\"features\").schema[0].metadata.\\\n",
    "                              get('ml_attr').get('attrs').get('binary')\n",
    "    # Numerical Features (得到連續型變數feature names)\n",
    "    numeric_metadata = predictions.select(\"features\").schema[0].metadata.\\\n",
    "                               get('ml_attr').get('attrs').get('numeric')\n",
    "    all_metadata = [m['name'] for m in binary_metadata + numeric_metadata]\n",
    "    \n",
    "    if cf == 'coefficient':\n",
    "        coefficients = np.array(model.coefficients).tolist() # For Logistic Regression\n",
    "    elif cf == 'features':\n",
    "        coefficients = np.array(model.featureImportances).tolist() # For tree\n",
    "        \n",
    "    coef_sumry = spark.createDataFrame([*zip(all_metadata, coefficients)],\n",
    "                                          ['feature', 'coefficients']).orderBy('coefficients')\n",
    "    \n",
    "    accuracy = eval_result('accuracy', pred)\n",
    "    pred_and_label = predictions.withColumn('label', col('label').cast(FloatType())).select('prediction', 'label')\n",
    "    metrics = MulticlassMetrics(pred_and_label.rdd.map(tuple))\n",
    "    #f1 = eval_result('f1', pred)\n",
    "    #precision = eval_result('weightedPrecision', pred)\n",
    "    #recall = eval_result('weightedRecall', pred)\n",
    "    \n",
    "    print(type(algo).__name__)\n",
    "    print('-' * 80)\n",
    "    print('Accuracy: {:5.2f}'.format(accuracy))\n",
    "    print('F1: {:11.2f}'.format(metrics.fMeasure()))\n",
    "    print('Precision: {:4.2f}'.format(metrics.precision()))\n",
    "    print('Recall: {:7.2f}'.format(metrics.recall()))\n",
    "    \n",
    "    pred_and_label = predictions.withColumn('label', col('label').cast(FloatType())).select('prediction', 'label')\n",
    "    conf_matrix = metrics.confusionMatrix().toArray()\n",
    "            \n",
    "    preds = predictions.select('label','probability').\\\n",
    "                        rdd.map(lambda row: (float(row['probability'][1]), float(row['label']))).\\\n",
    "                        collect()\n",
    "    \n",
    "    y_score, y_true = zip(*preds)\n",
    "    fpr, tpr, thresholds = roc_curve(y_true, y_score, pos_label = 1)\n",
    "        \n",
    "    # plot confusion matrix\n",
    "    trace1 = go.Heatmap(z=conf_matrix,\n",
    "                        x=['Not Subscribe', 'Subscribe'],\n",
    "                        y=['Not Subscribe', 'Subscribe'],\n",
    "                        showscale=False, colorscale='Picnic',\n",
    "                        name='Confusion_matrix')\n",
    "    \n",
    "    # plot roc curve\n",
    "    trace2 = go.Scatter(x=fpr,y=tpr,\n",
    "                        line=dict(color=('rgb(205, 12, 24)'), width=2))\n",
    "    \n",
    "    trace3 = go.Scatter(x=[0, 1], y=[0, 1],\n",
    "                        line=dict(color=('rgb(205, 12, 24)'), width=2,\n",
    "                                  dash='dot'))\n",
    "    \n",
    "    trace4 = go.Bar(y=list(map(lambda x: x['feature'], coef_sumry.select('feature').collect())),\n",
    "                    x=list(map(lambda x: x['coefficients'], coef_sumry.select('coefficients').collect())),\n",
    "                    orientation='h',\n",
    "                    marker=dict(color=list(map(lambda x: x['coefficients'], coef_sumry.select('coefficients').collect())),\n",
    "                                colorscale='Picnic',\n",
    "                                line=dict(width=0.6, color='black')))\n",
    "    \n",
    "    fig = tls.make_subplots(rows=2, cols=2, specs=[[{}, {}], [{'colspan': 2}, None]],\n",
    "                            subplot_titles=('Confusion Matrix',\n",
    "                                            'Receiver Operating Characteristic'))\n",
    "    fig.append_trace(trace1, 1, 1)\n",
    "    fig.append_trace(trace2, 1, 2)\n",
    "    fig.append_trace(trace3, 1, 2)\n",
    "    fig.append_trace(trace4, 2, 1)\n",
    "    \n",
    "    fig['layout'].update(showlegend=False, title='Model performance',\n",
    "                         autosize=False, height=900, width=1000,\n",
    "                         plot_bgcolor='rgba(240, 240, 240, 0.95)',\n",
    "                         paper_bgcolor='rgba(240, 240, 240, 0.95)',\n",
    "                         margin=dict(b=195))\n",
    "    fig['layout']['xaxis2'].update(dict(title='false positive rate'))\n",
    "    fig['layout']['yaxis2'].update(dict(title='true positive rate'))\n",
    "    fig['layout']['xaxis3'].update(dict(showgrid=True, tickfont=dict(size=10)),\n",
    "                                   tickangle=0)\n",
    "    py.iplot(fig)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
